---
title: "Lit Review for handling numerical ties in PCPs"
author: "Denise Bradford"
format: 
  pdf:
    classoption: ["11pt", "english", "singlespacing", "headsepline"]
    geometry:
        - paper=letterpaper
        - inner=2.5cm
        - outer=3.8cm
        - bindingoffset=.5cm
        - top=1.5cm
        - bottom=1.5cm
    include-in-header:
      text: |
        \usepackage{soul}
        \usepackage[dvipsnames]{xcolor} % colors
        \newcommand{\db}[1]{{\textcolor{Green}{#1}}}
        \newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
        \newcommand{\hh}[1]{{\textcolor{blue}{#1}}}
  html:
    css: edits.css
bibliography: thesis.bib
bibliographystyle: apa
filters:
   - latex-environment
commands: [db,svp,hh]
---

```{r include_packages, include = FALSE}
library(GGally)
library(palmerpenguins)
library(tidyverse)
library(ggpcp)
library(stats)
options(warn=-1)
require(gridExtra)
library(ggridges)
library(patchwork)
library(ggparallel)
theme_set(theme_bw())

penguin_scale_color <-   function(...) scale_color_manual(
    "Species", values = c("Gentoo" = "#540D6E", "Adelie" = "#219B9D", "Chinstrap" = "#FF8000", ...)
  )  
penguin_scale_fill <-   function(...) scale_fill_manual(
    "Species", values = c("Gentoo" = "#540D6E", "Adelie" = "#219B9D", "Chinstrap" = "#FF8000", ...)
  )  
```

# Introduction

<!--[I think the thing to do in the introduction is describe the problem of both large $n$ and large $p$ data; then state that your focus will primarily be on ways to work with data that has $p\geq 4$ and $n\geq 50$ to enhance our ability to visually detect patterns across multiple dimensions. You can introduce PCPs as a way to track a single observation across multiple dimensions, and then you can narrow down your focus into the problem of ties and how they disrupt the ability to keep track of a single observation.]{.svp}-->

[In modern data analysis, the complexity of datasets with large $n$ (number of observations) and large $p$ (number of variables) presents unique challenges. High-dimensional data, where $p \geq 4$ and $n \geq 50$, is increasingly common in genomics, finance, and social sciences domains. Analyzing such data often requires methods that enhance interpretability while preserving the intricate relationships within the data. However, the scale of observations and variables can obscure meaningful patterns and impede traditional visualization techniques.]{.db}

[Large $p$ datasets are particularly challenging because visualizing relationships across multiple dimensions often leads to information overload, occlusion, and cognitive strain [@keim2002]. Visual representations struggle to maintain clarity when $p \geq 4$, as patterns become increasingly difficult to discern in higher-dimensional spaces [@alber2020]. Similarly, large $n$ datasets exacerbate these issues by introducing visual clutter, making it harder to track individual observations [@heer2010]. These challenges require innovative visualization techniques that simplify complex data while preserving its structural integrity.]{.db}

[Parallel Coordinate Plots (PCPs) have become a powerful way to examine data with more than one dimension. PCPs show each observation as a polyline on parallel axes, letting analysts follow specific data points in multiple directions [@Inselberg1985]. By setting factors up as parallel vertical axes, PCPs make it easier to find patterns, trends, and outliers in large datasets.]{.db} These plots are especially good at showing relationships and trends between multiple factors. However, seeing ties and overlapping data points can be hard when many numbers are the same or very close. This can cause important information to be lost in the research. This study fixes the problems that regular PCPs have with finding numerical ties by adding a new method that makes it easier to tell the difference between data sets that overlap. We want to create a methodology that makes it easier to tell the difference between ties by using a standard delta difference approach. Our suggested methods should make it easier to understand data, leading to more accurate research and decision-making in large, complex datasets.

<!--The accuracy of data interpretation is highest when dealing directly with numerical values or one-dimensional visual representations [@spence1990]. That is becoming more and more impractical as the size and dimension of datasets continue to increase. To make judgments based on data, it is necessary to find methods of simplifying and consolidating datasets without sacrificing significant information. Data visualization, such as graphs or tables, is often an essential and invaluable intermediate step between raw data and an observer's decision-making process. An effective data visualization communicates the gist of a data set in a way that is easy for an observer to understand and evaluate; ideally, data visualizations slightly sacrifice accuracy (relative to a table of values) in favor of a greater understanding of the relationship between observations or variables. [I don't know that we need this paragraph here, at the moment. Don't get rid of it entirely, though, it's not bad -- just not quite on topic here]{.svp}-->

Most standard data visualizations work within the Cartesian coordinate system, with variables or functions of variables mapped to the x and y axis. Additional variables can be mapped to different properties of the plotted points, bars, or lines, and analysts can also create small multiples that show subsets of the data; even with these additions, viewers quickly become overwhelmed by the amount of information when more than p = 3 or 4 variables are shown (including the x and y coordinates). When it is necessary to understand the relationship between more than four variables, visualizations on a Cartesian grid no longer work as well; extensions to additional dimensions are also ineffective [@inselberg1997]. Other approaches are necessary when it is necessary to understand $p \geq 4$ dimensions of data. Here, we examine parallel coordinate plots (PCPs) as a solution to $p \geq 4$ dimensional data visualization and assess the impact of different modifications of PCPs on their effectiveness for visualizing high N and/or high p dimensional data. We specifically evaluate the ability of each PCP version to facilitate the identification of overall trends, outliers, and clusters within $N \geq 4$ dimensional data across different magnitudes of N (observations) and p (variables).

<!--Stimulus-responsive attention often arises when an observer recognizes that a particular graphical element in a stimulus signals valuable information, guiding further search to improve judgment accuracy. @spence1990 talked about the importance of visual psychophysics in figuring out how simple parts of graphs can be used to convey information. He stressed that how well we can understand graphs relies on our cognitive and visual abilities. Cleveland and McGill's study on graphical perception in 1984 found that people have different skill levels when decoding multidimensional data [@cleveland1984]. This shows that designs like generalized parallel coordinate plots must consider cognitive and perceptual variability. These plots show many dimensions within a single visual glyph, and the small changes between them make it much harder to understand complicated visual data. Additional studies, like @heer2010 and @simkin1987, built on these ideas by showing that mistakes happen at different points in information processing and that different graphs are better for different jobs depending on the user's accuracy. It was emphasized by @carswell1992 and @shah2011 that task difficulty and perceptual processes interact. These articles say differences in how people think and perceive things should be considered when making generalized parallel coordinate plots to show multidimensional data. [I don't know that we need this paragraph here, at the moment. Don't get rid of it entirely, though, it's not bad -- just not quite on topic here]{.svp} -->

# Parallel Coordinate Plots (PCPs)

Parallel coordinate plots (PCPs) leverage a projective coordinate [system, instead of a]{.svp} Cartesian coordinate system: each line in Cartesian space is a set of points in projective space, and each point in Cartesian space can be represented as a line in projection space [@Inselberg1985]. The result is that a single data point is [represented as a line that crosses each parallel axis representing a variable; clusters, then, appear as a group of lines which have similar paths.]{.svp}

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-ggpairs
#| layout-valign: bottom
#| fig-cap: "A generalized pairs plot"
#| fig-width: 8
#| fig-height: 8

penguins_clean <- na.omit(penguins)

suppressWarnings(
  ggpairs(penguins_clean[,c(3:6)], 
          lower = list(continuous = wrap("points")),
          mapping = aes(color = penguins_clean$species),
          columnLabels = c("bill_length[mm]", "bill_depth[mm]",
                           "flipper_length[mm]", "body_mass[g]" ), 
          diag = list(continuous = wrap("blank")),
          #diag = list(continuous = wrap("densityDiag", alpha = 0.5),
          #           discrete = "barDiag")
  )+
    theme(axis.text.x = element_text(angle=45)) +
    penguin_scale_color()
)
```

@fig-ggpairs is a [**scatterplot matrix**, which shows pairwise relationships between variables in Cartesian space]{.svp}. 
It shows how the variables for three kinds of penguins (Adelie, Chinstrap, and Gentoo) are distributed and how they are related.
<!-- Histograms and density plots [provide univariate distribution information to supplement the bivariate scatterplots for numeric variables]{.svp} (bill length, bill depth, flipper length, and body mass).  -->
<!-- The density plots show clear trends for each species, regarding the length of the bills and flippers, where Gentoo penguins tend to have higher values.  -->
<!-- In the boxplots that show body mass and flipper length, outliers are clear because they show numbers very different from the rest of the data.  -->

[While some of the information is obscured due to overplotting, the overall relationship between variables is relatively clear and is reinforced by the numerical values displayed in the corresponding pair of variables across the diagonal.]{.svp} 
Overall, the matrix-style plot does an excellent job of showing the links within and between variables. 
[The major downside to scatterplot matrices is that it is not possible to easily connect a point in one scatterplot to a corresponding point in another; the data representation makes it difficult to get a sense of the multivariate relationships beyond any combination of $p=2$ variables.]{.svp}

```{r}
#| echo: false
#| warning: false
#| label: fig-pcp
#| layout-valign: bottom
#| fig-cap: "A parallel coordinate plot"
#| fig-width: 8
#| fig-height: 4
#| cache: false

penguins %>%
  pcp_select(3:6) %>%
  pcp_scale(method="uniminmax") %>%
  ggplot(aes_pcp()) +
  geom_pcp_axes() +
  geom_pcp(aes(colour = species), alpha = 0.5) + 
  theme_pcp() + 
  penguin_scale_color() + 
  theme(legend.position = "bottom", legend.direction = "horizontal")

```

In @fig-pcp, each line shows a different penguin, and the colors of the lines show the species. 
The Gentoo (blue) tends to have higher body mass and flipper length values, while the Adelie (red) tends to have lower values for these measurements. 
[There is an overall negative relationship between flipper length and bill depth, indicated by the "X" shape of the lines, but within each species, the overall relationship between flipper length and bill depth is positive, as indicated by the largely parallel block of lines of each color. ]{.svp}
[Even in projection space, we can see Simpson's paradox at work; the second plot in the third row of @fig-ggpairs shows the same basic information.]{.svp} 
[While there is a significant amount of information obscured due to overplotting, it is easier to connect observations across $p>2$ vertical axes, providing a greater ability to visualize more than two dimensions.]{.svp} 
[At each vertical axis, the plot resembles a rug plot,]{.svp}[@cleveland1993]{.db} [which can provide some information about the distribution of the variable, but is less direct than a continuous density plot.]{.svp} 


<!-- It's hard to directly find outliers in @fig-pcp because the merging lines make it hard to see individual differences.  -->

<!-- The plot doesn't show specific distribution shapes like a histogram or density plot, but how the lines are grouped at specific value ranges shows how each species generally behaves.  -->

<!-- The slope and intersection patterns of the lines suggest that the variables are related.  -->

<!-- Note that the plot demonstrates that if the length of a bird's bill goes up, other variables may go up or down similarly, based on the species.  -->

<!-- Overall, the plot works well for seeing broad patterns and trends across species, but it could be better at picking out specific data points or weak connections because it's too crowded on the screen. -->

<!-- For high-dimensional data, parallel coordinate plots (PCPs) are a standard visualization tool whereby data points are shown as a line intersecting a series of vertical axes representing the different variables in the dataset, therefore indicating a dimension.  -->

[As originally proposed,]{.svp} PCPs [could]{.svp} be difficult to interpret[, in part because PCPs were initially defined only for numeric variables; extensions which treated categorical variables as numeric suffered from]{.svp} overplotting [, as different lines converge on a single point and then diverge again, destroying the ability to trace a single observation through the categorical-turned-numerical axis]{.svp}.
[The `ggpcp` package [@vanderplas2023] introduced a new way to handle categorical variables, dividing the axis up into "boxes" and ordering observations within those boxes; for relatively small $n$, this preserves the ability to follow single observations across the plot, and for larger $n$, a series of lines moving together converge to form an approximate hammock plot .]{.svp} 
[A demonstration in @fig-ggpcp replicates @fig-pcp with the addition of a categorical species axis on each side of the plot.]{.svp} 
[In the InfoVis community, other modifications to parallel coordinate plots have been proposed: smoothed lines, density-based PCPs[@density-pcp], bundling of similar points, and other modifications such as interactivity [@johansson2015] may support identification of clusters and outliers in multidimensional space.]{.svp}

```{r}
#| echo: false
#| warning: false
#| label: fig-ggpcp
#| layout-valign: bottom
#| fig-cap: "A generalized parallel coordinate plot, with species on the left and right of the plot; observations are ordered on the right side based on the value of body_mass_g, and on the left side based on the value of bill_length_mm. Translucent lines reduce the impact of overplotting and allow perception of the Adelie lines (which are plotted first) even as the Gentoo and Chinstrap lines are plotted on top. With this treatment, we can see that the strong positive relationship between bill depth and flipper length in Gentoo penguins is much less pronounced in Adelie and Chinstrap penguins; there are many more line crossings in both species, which suggests a more moderate relationship."

penguins %>%
  pcp_select(species, 3:6, species) %>%
  pcp_scale(method="uniminmax") %>%
  pcp_arrange(method = "from-left") %>%
  ggplot(aes_pcp()) +
  geom_pcp_boxes() + 
  geom_pcp(aes(colour = species), alpha = 0.5) + 
  geom_pcp_labels() +
  theme_pcp() +
  penguin_scale_color() + 
  penguin_scale_fill() + 
  guides(color = F)
```

<!-- - lines overlap and need to be distinguishable, and when there are large datasets, even non-identical segments can be hard to identify because of the number of line segments.  -->

<!-- [PCP] -->

<!--It needs to improve its interpretability by having clutter and overlapping lines while handling big datasets. Generalized PCPs (GPCPs) introduced several different methods: By including axis reordering, bundling, and dimensionality reduction to solve these problems, generalized parallel coordinate plots (GPCPs) expand the conventional PCP.-->

<!-- These improvements in clarity and usability of PCPs utilizing data pattern highlighting and visual clutter reduction help [@density-pcp], for example, present several approaches to bundle comparable trajectories in PCPs, so minimizing overlap and improving pattern identification; Johannsen et al. address the advantages of dynamic axis reordering to highlight different data characteristics [@johannsen2012]. -->

## Why?

[Plots using]{.svp} Cartesian coordinates are straightforward, in part because they are [commonly encountered and taught in grade school]{.svp}. 
[Unfortunately, plots using Cartesian coordinates, such as scatterplots, are limited to two variables displayed using spatial dimensions, which are perceived more accurately than other aesthetic mappings such as color and shape [@cleveland1984].]{.svp}
[Projections of three-dimensional scatterplots can be created, but these charts can be difficult to read and interpret; interactive 3D scatterplots still suffer from the loss of information inherent to 2D projection on a screen, but allow for some sense of the full shape of the data.]{.svp}
[Data sets with more than three numerical dimensions are extremely common, but cannot be easily shown using Cartesian coordinates; analysts must resort to strategies like tours [@tourr], dimension reduction [@tsne;@abdiPrincipalComponentAnalysis2010], or plotting bivariate relationships between variables in order to visualize these data sets.]{.svp}
They make it easier to tell the difference between different information points and their exact values by displaying data points in an easily understood manner. 
Cartesian coordinates are an excellent method to present data clearly because they can handle up to three dimensions. 
They also need help with growth because adding more dimensions requires a lot of subplots or complicated three-dimensional plots, which can get boring and challenging to follow. 
Cartesian graphs additionally can take up a lot of room, and for data with more dimensions, they usually need more than one plot.

Parallel coordinate plots (PCPs) are an excellent way to show data with many dimensions since each is shown on its plane. 
<!-- This means that complex relationships between different factors can be studied repeatedly. [repeatedly?? Not sure what you mean by this]{.svp}-->
PCPs are useful for drawing attention to trends, clusters, and outliers and efficiently show many variables in a small area. 
<!-- [are great for is very informal language -- can you rephrase this to be more formal? e.g. PCPs effectively show bivariate relationships, clusters of similar data points across multiple variables, and outliers, as shown in  ...., and then provide a figure showing how you can see those qualities within a PCP?]{.svp} -->
However, as with their Cartesian equivalents, PCPs are vulnerable to overplotting, [becoming]{.svp} difficult to read [and]{.svp} interpret when N is large. 
[In addition, PCPs are a much less familiar form of visualization than scatterplots; there is an initial adjustment period required in order to understand which features of a PCP correspond to familiar features of a scatterplot, as shown in @fig-features]{.svp}. 
[This lack of familiarity may be the biggest drawback of PCPs; however, there are ... studies which explore this issue.]{.svp} [XXX are there studies that explore how quickly people acclimate to PCPs? If not, we should consider running one!]{.svp}

# Variations on Parallel Coordinate Plots

Since @Inselberg1985 introduced parallel coordinate plots (PCPs) in 1985, considerable advances have been made to address the original method's framework and improve its capacity to depict high-dimensional data effectively.
The enhancements include changes to visual representation, handling various kinds of interactive data elements, and incorporating advanced computational approaches to increase the clarity and interpretability of the visualizations. 
[In this section, we examine some of the modifications proposed to enhance PCPs after their original introduction, as well as variations on PCPs which evolved in parallel for e.g. categorical variables.]{.svp}

## Categorical and Hybrid PCPs

Traditional PCPs were primarily designed to collect continuous data.
However, real-world datasets often contain a mix of continuous, ordinal, and categorical data.
To solve this, changes have been made to display mixed-type data within the same PCP simultaneously.
Categorical data, for example, can be visually differentiated using unique colors, symbols, or segmented lines, although continuous variables are still represented by standard lines linking the axes.
Categorical Parallel Coordinate Plots (CPCPs) were created to handle the challenge of representing categorical data in PCPs, which are not suitable for continuous axis representation.
Specific strategies, like adjustments to the ends of links, have been suggested to enhance how category and numerical values are connected visually.
This strategy enhances the interpretability of mixed data by modifying the plotting criteria for axes representing category variables.

CPCPs use discrete axis segments or unique markers for each category, transforming categorical variables into distinguishable visual elements.
@siirtola2006 introduced axis segmentation, using markers and spacings to represent categorical values distinctly, improving categorical data visualization in PCPs.
@inselberg2009 enhanced this representation by introducing specific encodings, which allowed categories to be differentiated visually, preserving the PCP's interpretative power.
@johansson2015 explored alternative segmentation methods to minimize visual clutter, enabling effective handling of multiple categorical values on the same axis.

CPCPs adjust by organizing categorical axes more clearly or using symbols and colors to represent categories for easier interpretation. [Is this actually a CPCP development, or did it just develop alongside the idea of CPCPs? It seems like it would apply to all PCP-variants.]{.svp}
[@Wegman:1990]{.db} <!--[Use the citation -- @XXX]{.svp}--> suggested using symbols and color codes to represent categories, improving how categorical data is understood in parallel coordinates.
[@holten2009]{.db} <!--[Use the citation -- @XXX]{.svp}--> introduced color gradients for categories to facilitate distinguishing between multiple categorical values, especially in the analysis of intricate datasets.
[@leblanc1990]{.db} <!--[Use the citation -- @XXX]{.svp}-->  highlighted the significance of arranging axes in CPCPs, positioning categories with strong relationships closer together to assist users in making meaningful comparisons.

```{r}
#| echo: false
#| warning: false
#| label: fig-hybrid
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Categorical and Hybrid in PCP"

# Remove rows with missing data from the dataset
penguins_clean_cat <- penguins %>%
  na.omit() %>%
  select(-year)

# Convert categorical variables to factors
penguins_clean_cat <- penguins_clean_cat %>%
  mutate(island = as.factor(island),
         sex = as.factor(sex),
         species = as.factor(species))

# Create a parallel coordinate plot using both numeric and categorical variables

penguins_clean_cat %>%
  pcp_select(2:7) %>%
  pcp_scale(method="uniminmax") %>%
  # pcp_arrange() %>%
  ggplot(aes_pcp()) +
  geom_pcp_axes() +
  geom_pcp(aes(colour = species)) +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Categorical and Hybrid PCP",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  facet_wrap(~sex) +
  theme(axis.text.x = element_text(angle=45)) +
  penguin_scale_color()
```

The Generalized Parallel Coordinate Plot (GPCP) is a modified version of the original PCP that introduces nonlinear changes to the axes for more complex data representation.
These can show more complicated relationships in the data that might not be visible in a regular PCP.
Nonlinear scales such as logarithmic and exponential transformations are possible.
@wegman1997a provide a comprehensive discussion of the GPCP idea stating, "The parallel coordinate plot is a geometric device for displaying points in high-dimensional spaces, in particular, for dimensions above three. 
As such, it is a graphical alternative to the conventional scatterplot."
The authors enable the visualization of data distributions, correlations, clusters, and higher-dimensional structures by connecting Cartesian and parallel coordinate representations.
The "grand tour" idea is important for seeing how dynamic high-dimensional structures change over time because it allows data analysts to examine very large and very complex data sets. 
This method avoids the problems of axis adjacency in PCPs by constantly rotating the data axes. 
It ensures that every possible combination of variables is explored dynamically.
These new ideas greatly improve the ease and usefulness of PCPs, making them more useful in many situations and giving us tools for exploring and analyzing data in the modern world, [@wegman1997].


### Categorical Variants of PCPs

[Parallel sets plots are similar to PCPs, but are designed for categorical variables.]{.svp}
Parallel sets are a technique for visualizing categorical data with great promise for assisting visual analytics. 
Among their various important characteristics and improvements are frequency-based representation, flexibility to mixed data, quality measurements for improved visualization, interactive elements like axis reordering, and dynamic filtering.

For frequency-based representations, parallel sets replace individual data points, abstracting data to emphasize linkages. 
The SET-STAT-MAP system is flexible for mixed-data situations like environmental and policy studies because it expands parallel sets to include numerical and spatial data, @wang2022set. 
Quality measures, including overlap, ribbon width variance, and mutual information, optimize dimension and category ordering and improve visualizations' interpretability.

Parallel sets depend on interactivity to allow users to investigate data relationships more naturally. 
These tools let users create aesthetically pleasing layouts that best fit their application. 
Across many fields, including meteorology, consumer behavior study, marketing, and resource management, parallel sets find application.

@dennig2021parsetgnostics introduced "ParSetgnostics," a set of quality metrics to reduce visual clutter in parallel sets​. 
Metrics like overlap, ribbon width variance, and mutual information optimize dimension and category ordering, significantly improving visualizations' interpretability; for instance, applying these metrics reduced clutter by up to 81% in test cases.
Parallel sets are also useful for instruction and analysis since their combination of statistical summaries and category flow analysis reveals a deeper understanding. 
Combining categories and automating axis selections guarantees that parallel sets stay scalable, therefore matching the aim of reducing the number of possible combinations produced on display without compromising data integrity.


Hammock plots [are similar to]{.svp} parallel coordinate plots, [but were designed to show categorical data [@schonlauVisualizingCategoricalData2003]]{.svp}. 
Hammock plots represent a breakthrough in data visualization, particularly for managing numerical and categorical data. 
While allowing mixed data types, they are meant to show relationships between categorical variables visually. 
Especially in health science datasets, key characteristics of Hammock plots include their capacity to simplify complicated relationships and lower visual clutter by using "bandwidth" representations to highlight links between categories.

When it comes to multidimensional categorical data, hammock plots show a more compact representation than mosaic plots, lowering the cognitive load on the observer and surpassing conventional tools like bar charts. 
These benefits are absolutely vital in disciplines including health research, where datasets can comprise mixed numerical and categorical variables needing simultaneous display.

@symanzik2018 showed the Titanic dataset using hammock plots, arguing that "Hammock plots allow for line alignment representing similar or identical values." 
This makes understanding the data in parallel coordinate space easier and gives you a better sense of the different data types and their groups. 
They do say, though, that Hammock plots "may add visual clutter in cases lacking categorical data distinctions," which is meant to stress the problems that might arise when they are used with continuous variables.

@kavvadias1996 use a decomposition method called "Hammock-on-ears" to work on bigger graph-theoretic problems, with the goal of enhancing the ability to find ties in large datasets. 
This shows that Hammock plots can handle small amounts of complex data. 
However, they warn that "the method may require significant computational resources," which means it might not work for complex or larger-scale tasks. 
<!--[Actually, it means the opposite - if you don't have the computer power, you can't use the method for big datasets, but it probably works for smaller ones]{.svp}-->

@pilhofer2012 show that Hammock plots "manage clusters and resolve data ties" well, especially when comparing data clusters.
Key strategies comprise clustering visualization, which employs algorithms to reorder rows and columns, obtain a pseudo-diagonal form, and resolve data connections via optimal sorting algorithms like the Weighted Bertin Classification Criteria (WBCC). 
In graphical displays, these techniques minimize crossing lines or misalignments and maximize concordance.
Optimized displays of clustering results, using the “Ecotest dataset,” show how ordering methods enhance cluster identification, @pilhofer2012a.
The paper also offers an evidence-based method, using measures such as the Bertin Classification Criteria (BCC) to evaluate graphical order, supported by computational tests.
Both theoretical considerations and actual data support the assertions regarding Hammock plots controlling clusters and tying off conflicts. 
<!--[Show how? Is there an experiment? Or is this just a claim based on intuition?]{.svp}-->
<!--This is especially helpful in datasets that contain overlapping lines, which can be hard to due to  [Why are they hard to see?]{.svp}  because Hammock plots make it easier to separate the data. -->

Still, @pilhofer2012 points out a problem: "The focus on clustering may introduce unnecessary complexity in dispersed data." 
They suggest that Hammock plots work best for datasets with clear clusters.
Both visual and computational methods help to find definite clustering in the content. 
The main technique is pseudo-diagonalization, in which rows and columns of a data matrix are rearranged to visually show clusters by combining similar entries. 
This technique is compared by a top-down partitioning method, which clusters the data using measurements such as Kendall's $\tau$ or the Bertin Classification Criteria (BCC), determining ideal cut spots in rows and columns. 
The BCC, which computes the difference between conforming and non-conforming pairings to gauge the alignment of matrix entries with a pseudo-diagonal form, is one of the metrics to evaluate clustering clarity. 
The Weighted Bertin Classification Criteria (WBCC) emphasizes the proximity of data points and improves this by using distance-based weights; the Bertin Classification Index (BCI) scales the BCC to gauge the strength of the link between clusters. 
BCI provides a consistent approach to evaluating datasets, with values ranging from 0 (perfectly aligned clusters) to 1 (indicating no clustering). 
The main evaluation criterion for clarity is the degree to which the matrix resembles a block-diagonal structure; low BCI values imply unambiguous clusters, while high values indicate indistinct or overlapping groupings. 
The paper also underlines the need for visual inspection, in which visually distinct and well-separated blocks in matrix plots—such as those shown in the Ecotest dataset examples—characterize clear grouping. 
Still, clarity is very arbitrary since it depends on thresholds selected for BCI and the interpretability of the produced visuals. 
This method detects and assesses clusters by combining empirical visualization with quantitative measurements.

<!--[Do they provide a metric for identifying such datasets? How clear is clear?]{.svp}.-->

[The `ggparallel` package]{.svp} [@hofmannCommonAnglePlots2013]{.db} [provided a grammar-based interface]{.svp} for visualizing [multivariate categorical]{.svp} data, implementing Hammock plots [@schonlau2024].
This makes them better suited for data that has both category and numerical relationships. 
They say this grammar-based adaptation retains the established behaviors of parallel coordinate while giving you more options for dealing with edge cases like categorical ties. 
<!-- This makes Hammock plots more flexible for use with large datasets. [Please don't conflate PCPs and hammock plots -- they're similar, but not the same thing]{.svp} 
[Also, please don't cite arxiv papers when a peer-reviewed version exists -- e.g. Penguins go Parallel]{.svp}

 [I would like to get rid of the section below and highlight the work "Hammock Plots: Visualizing Categorical and Numerical Variables" by Schonlau]{.db}

@vanderplas2023 investigate more deeply how useful Hammock plots are for dealing with numerical ties in categorical datasets. 
They show that "Hammock plots reduce overlap in tied data," which makes it easier to understand parallel coordinate plots. [I can't find this quote anywhere in the paper -- are you actually copying quotes from the paper when you quote things??? Please go back and review all quotes and make sure you can put a page number that's accurate -- otherwise, remove the quotes and rephrase -- but make sure you're accurate to the meaning!!.]{.svp}
However, they also admit that these plots are "less effective for continuous data without ties or when categorical values are minimal," [This phrase does not exist in the paper!]{.svp} suggesting that Hammock plots offer limited benefits in uniformly distributed datasets. -->

[@hofmannCommonAnglePlots2013 worked on Common Angle Plots, and Schonlau added Hammock plots to improve categorical data display.
However, they approached the problem in different ways. 
@schonlau2024 Hammock Plots: Visualizing Categorical and Numerical Variables generalizes parallel coordinate plots by swapping lines with box elements. 
The width of the boxes shows the number of observations, so they can be used for both categorical and mixed data. 
By changing traditional Hammock plot designs, Schonlau deals with perceptual problems like the reverse line width illusion. 
@hofmannCommonAnglePlots2013 also addresses the line width illusion, proposing Common Angle plots that use a constant angle to mitigate the line width illusion while maintaining visual continuity of the categories. 
Hofmann's Common Angle Plots are unique because they use consistent angular links to fix problems when line slopes or widths are different. 
This ensures that the visualizations are accurate. 
Schonlau stresses the importance of being flexible with different data types, while Hofmann uses a consistent angular structure to clarify interpretations. 
Together, these approaches show a shared desire to make categorical and mixed data displays more accurate and useful, using different but complementary methods.]{.db}

```{r}
#| echo: false
#| warning: false
#| label: fig-hammock
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Hammock Plot [@schonlau2024]"
#|   - "Common Angle Plot [@hofmannCommonAnglePlots2013]"
#| fig-width: 8
#| fig-height: 4
#| layout-ncol: 1


# Filter out NA values for easier plotting
penguins_clean <- penguins %>%
  na.omit() %>%
  mutate(
    species = factor(species),  # Encode species as numeric
    island = factor(island),    # Encode island as numeric
    sex = factor(sex),
    year = factor(year))

# Create Hammock Plot
ggparallel(list("year", "island", "sex", "species"),
           data = as.data.frame(penguins_clean),
           method = "hammock",
           ratio=0.25) +
  labs(title = "Parallel Coordinate Plot for Penguin Year, Species, Island, and Sex") +
  penguin_scale_color() +
  theme_minimal()


# Create Hammock Plot
ggparallel(list("year", "island", "sex", "species"),
           data = as.data.frame(penguins_clean),
           method = "angle",
           ratio=0.25) +
  labs(title = "Parallel Coordinate Plot for Penguin Year, Species, Island, and Sex") +
  penguin_scale_color() +
  theme_minimal()

# # Create Parallel Coordinate Plot
# parallel_plot <- ggpairs(penguins_clean, columns = c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"),
#                          aes(color = species, alpha = 0.5)) +
#   theme_minimal() +
#   labs(title = "Parallel Coordinate Plot for Penguin Measurements")
# 
# # Combine the two plots for side-by-side comparison
# combined_plot <- hammock_plot + parallel_plot +
#   plot_layout(ncol = 2) +
#   plot_annotation(title = "Comparison of Hammock Plot and Parallel Coordinate Plot")
# 
# # Print the combined plot
# print(combined_plot)

```

@fig-hammock is a Hammock plot that shows the relationship between two categorical variables, gear and cylinders (cyl). 
Each color-coded band represents a different category within these variables. 
The plot provides frequency but lacks precise counts or numeric values. 
Overplotting is evident, making it difficult to trace individual pathways. 
The plot shows the general distribution and relationships between gear and cylinder categories, but lacks numeric relationships or exact proportions.

## Bundling and Curving

Bundling and curving techniques have been introduced to handle visual clutter, especially with high-dimensional data. 
These techniques group similar paths, making patterns more visible and reducing overplotting. 
Bundling and curving significantly reduce clutter, making spotting overarching trends and common patterns easier. 
@holten2009 note that bundling offers a compelling solution to mitigate the chaos often found in high-dimensional visualizations.

<!--Holten and van Wijk pioneered edge bundling for PCPs, which visually aggregates similar paths to reduce clutter.--> 
@mcdonnell2008 furthered this approach by introducing curvilinear PCPs, where curved lines help distinguish intersecting paths for clearer visual analysis. 
@johansson2015 evaluated the effectiveness of bundling and curving in PCPs, establishing criteria for when these modifications enhance interpretability.

Curved lines can be altered to vary in curvature dependent on data attributes, which improves visual separation and pattern recognition, particularly in scenarios with strongly correlated dimensions. 
Curving in PCPs visually separates overlapping lines, making data relationships more discernible. 
It improves line separation by reducing intersections, making it easier to differentiate between trajectories. 
Curved lines are more aesthetically pleasing and reduce cognitive load, enhancing readability. 
They simulate a more three-dimensional space within a two-dimensional PCP, making it easier to perceive data points' relative positioning.
Research shows curving lines reduces visual clutter and improves data interpretation [@fua1999, @qu2007]. 
This strategy has improved PCPs' ability to visualize complicated datasets with overlapping points [@kachhway2013].

```{r}
#| echo: false
#| warning: false
#| label: fig-bundling
#| layout-valign: bottom
#| fig-subcap: 
#|   - "A Bundling and Curving Parallel Coordinate Plot"

penguins_clean <- penguins %>%
  na.omit() %>%
  mutate(species = as.factor(species)
         ) %>%
  select(-island, -sex, -year)


# Create a parallel coordinate plot with bundling and curving
ggparcoord(
  penguins_clean,
  columns = 2:5, # Using the numeric columns
  groupColumn = 1,
  alphaLines = 0.5,
  splineFactor = 5
) +
     theme_minimal() +
     labs(title = "Parallel Coordinate Plot with Bundling and Curving",
                   x = "Variables",
                   y = "Scaled Value") +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle=45)) +
    penguin_scale_color()
```

In @fig-bundling, the PCP modification demonstrates adding bundling and curving, which makes the lines smoother, less distracting, and better able to tell the patterns between the species apart. 
It's easier to see general trends within each species when the data is bundled, but it still doesn't show specific distribution shapes like a histogram might. 
Bundling makes it harder to see outliers in this image because the extreme values are "pulled" into the main flow, making it harder to see individual deviations. 
The curved lines make the relationships between the variables stand out, indicating that specific measurements may go up or down together depending on the species. 
Still, they make it harder to see small changes between individual points.

@moustafa2011 combine parallel coordinate plots (PCPs) with parallel coordinate density plots (PCDPs) to reduce clutter when working with big datasets. 
As part of Moustafa's methodology, the standard PCP is mutated into a density plot. 
This creates plot areas with more observations that stand out and reduce visual clutter.

The new approach uses density estimation methods to transform the PCP image based on polylines into a continuous, smooth depiction of data density. 
This change shows how to see groups and trends usually hidden in regular PCPs. 
@moustafa2011 has interactive parts that let users experience different data dimensions in real-time. 
This makes PCPs even better at analysis. With its interactive nature, people who aren't experts can use this method confidently and successfully.

Moustafa's research indicates that to deal with even larger datasets more efficiently, future efforts should focus on enhancing density estimation methods. 
These methods can be used in biology and finance to demonstrate their usefulness and adaptability for examining real-world data.

Bundling groups with similar paths may also obscure individual data points or outliers, which can be crucial in some analyses. 
The clarity gained in bundle simplification comes at the cost of data precision, warns McDonnell and Mueller, pointing out that outliers or unique data paths might be lost [@mcdonnell2008].

### Enhanced Color Encoding and Shading

Visual upgrades like color coding, opacity modifications, and changing line thickness have been implemented to solve overplotting concerns, particularly with massive datasets.
Significant data trends can be highlighted using different colors or modifying line opacity in response to data density, with less relevant information deemphasized.
This approach helps users identify trends and outliers that might otherwise be hidden.
Line thickness can be adjusted to reflect other variables, such as data frequency or confidence intervals, adding depth to the visual depiction.
Such multi-layered visualization techniques provide an additional layer of information to classic PCPs

In their 2013 work "State of the Art of Parallel Coordinates," @heinrich2013 give a full overview.
They discuss the different methods used to avoid overplotting and improve data interpretation.
They discuss how to use alpha blending, which changes the transparency of lines to clear up space, and interactive line manipulation methods that let users explore different data points in real-time.
These tips are important for making parallel coordinate plots easier to read, especially when working with big datasets.

In their 2008 study, "Extensions of Parallel Coordinates for Interactive Exploration of Large Multi-Timepoint Data Sets," @blaas2008 stress how important it is to have visual traits that can be changed, such as the colors and opacity of clusters.
Their method lets users change these settings in real-time, which makes it easier to focus on specific data features and control line density.
In the same way, @raidou2015, in "Orientation-Enhanced Parallel Coordinate Plots," talk about how different levels of opacity and color can help draw attention to patterns, show line densities, and allow for interactive data study.
In their work, they also talk about how to deal with the problem of overplotting in complex datasets by smoothing and averaging polylines.

In their paper "Visual Clustering in Parallel Coordinates," @zhou2008 make another important addition by suggesting a tool that lets users choose color and opacity to draw attention to clusters in the data.
This method helps tell the difference between data groups and changes the shape of curves to make things clearer.
In their paper "Hierarchical Parallel Coordinates for Exploration of Large Datasets," @fua1999 also talk about ways to reduce visual clutter by changing the thickness of lines and using hierarchical data representations.
These methods make it possible to effectively visualize data density, which gives you a better look at the trends hidden in big datasets.

These studies show that changing how parallel coordinate plots look, such as color coding, adjusting brightness, and moving lines around, can make them much easier to read and understand.
These methods are significant for dealing with thick and overlapped data, making parallel coordinate plots a more helpful tool for studying data in multiple dimensions.

By adding color gradients and shading, PCPs can encode additional data attributes, such as density or frequency, improving the ability to spot trends and correlations.
Color gradients and shading effectively encode additional variables, allowing more complex data insights to be derived visually.
Color encoding adds a new layer of perception, turning a purely structural plot into a multi-dimensional analysis tool, [@theus2008].

@theus2008 introduced color-coded parallel coordinates, which allow users to map additional variables using color and thereby increase interpretive power.
@bertini2005 applied density shading techniques to PCPs to make frequent patterns in large datasets stand out more prominently.
@novotny2006 developed opacity and color-blending techniques in PCPs, which help understand overlapping patterns more intuitively.

Heavy reliance on color may lead to visual strain, especially in cases where multiple gradients overlap or in users with color vision deficiencies.
Bertini et al. noted, "When too many colors are applied, the encoding becomes confusing, and can overwhelm rather than enhance the viewer's understanding."

```{r}
#| echo: false
#| warning: false
#| label: fig-encoding
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Enhanced Color Encoding and Shading in PCP"

# Create a parallel coordinate plot with enhanced color encoding and shading
ggparcoord(
  data = penguins_clean,
  columns = 2:5, # Using the numeric columns
  groupColumn = 1,
  scale = "uniminmax", # Scale columns to range between [0, 1]
  alphaLines = 0.5, # Set transparency for lines
  #showPoints = TRUE, # Show points on lines
  mapping = ggplot2::aes(color = species), # Color lines by species
  shadeBox = NULL # Use to add shading to enhance visual distinction between variables
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Enhanced Color Encoding and Shading",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle=45))
```

In @fig-encoding, the parallel coordinate plot uses enhanced color coding.
It might have changed the line density, which makes the differences between species (Adelie is red, Chinstrap is green, and Gentoo is blue) more obvious across the variables (bill length, bill depth, flipper length, and body mass).
The plot does a good job of showing general trends for each species.
For example, the Gentoo tends to have higher flipper length and body mass values, while the Adelie tends to have lower values.
Overplotting is still a problem, especially near the middle ranges of each axis, which hides small differences and makes it hard to find outliers directly.
The plot doesn't show individual distribution shapes, but how the lines are grouped on each axis suggests that each species has central tendencies.
The general trends of each species' lines suggest relationships between numerical factors, but it still needs to be easier to figure out individual correlations because they overlap.
This graph gives a broad picture of trends across species, but it could be better at picking out specific data points or rare observations.

## Dimension Reduction Techniques

Dimensionality reduction techniques such as Principal Component Analysis (PCA) and Independent Component Analysis (ICA) are frequently used to improve the readability of PCPs before charting. 
These strategies help to reduce the number of dimensions while retaining the most informative parts of the data. 
PCPs can be used to visualize reduced dimensions, providing a better understanding of high-dimensional interactions. 
Axis ordering is another important feature that has been adjusted. 
The relationships between variables can be highlighted by sorting the axes according to measurements such as correlation or mutual information. 
Automatic axis arrangement techniques have been developed to reduce line crossings, making recognizing patterns easier.

The article "Orientation-Enhanced Parallel Coordinate Plots" by @raidou2015 describes a new way to make parallel coordinate plots (PCPs) more accessible to read and understand. 
The authors suggest a method that uses input about direction to solve the clutter and overlap problems with regular PCPs. 
This method changes the direction of the plot axes on the fly by applying both automatic orientation and user-interactive adjustments. 
This method reduces visual noise and makes data patterns and correlations more visible. 
This will make it easier for users to study and analyze large datasets.

The suggested orientation-enhanced PCPs are designed with the user's needs at the forefront, offering a more informative visual representation. 
Several preprocessing steps, including principal component analysis (PCA) and multi-dimensional scaling (MDS), were employed to determine the optimal axis orientation. 
This ensures that data clusters are maximally separated and data lines are minimally overlapped. 
The interactive features allow users to adjust the orientation according to their preferences and requirements, enhancing the usability and adaptability of the plots for research needs [@raidou2015].

Dimension reduction has become critical for managing PCPs with high-dimensional data. 
Techniques such as principal component analysis (PCA) and clustering help to focus on the most significant data features. 
Reducing dimensions simplifies analysis, enabling analysts to focus on the most important features while avoiding overwhelming visual data. 
Dimension reduction helps to retain essential information without drowning the user in less relevant details [@wegman1997].

@wegman1997 applied PCA within PCPs, enabling users to represent key data features while minimizing less relevant dimensions. 
@guo2012 utilized clustering and hierarchical dimension reduction techniques to streamline high-dimensional datasets, enhancing usability without sacrificing detail. 
@yang2017 demonstrated the integration of non-linear dimension reduction methods within PCPs to capture complex relationships in data more effectively.

Dimension reduction techniques like PCA or clustering can inadvertently remove nuances or minor patterns that might be relevant in specific cases. 
@guo2012 caution that, while beneficial, these methods might lead to information loss, potentially masking relationships only visible in higher dimensions.

```{r}
#| echo: false
#| warning: false
#| label: fig-pca
#| layout-valign: bottom
#| fig-subcap: 
#|   - "A PCA in Parallel Coordinate Plot"


# Remove rows with missing data from the dataset
penguins_clean <- penguins %>%
  na.omit() %>%
  select(-island, -sex, -year)

# Perform PCA on the numeric columns
penguins_pca <- prcomp(penguins_clean %>%
                         select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g),
                       center = TRUE, scale. = TRUE)

# Create a data frame with the PCA results
penguins_pca_df <- as.data.frame(penguins_pca$x)
# Add species as a categorical variable to the PCA data frame
penguins_pca_df$species <- penguins_clean$species

# Create a parallel coordinate plot using ggparcoord for the PCA results
ggparcoord(
  data = penguins_pca_df,
  columns = 1:4, # Using the PCA components
  groupColumn = 5,
  scale = "uniminmax", # Scale columns to range between [0, 1]
  alphaLines = 0.3, # Set transparency for lines
  #showPoints = TRUE, # Show points on lines
  mapping = ggplot2::aes(color = species) # Color lines by species
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with PCA elements",
                y = "Scaled PCA Values",
                x = "Principal Components") +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle=45))

```

In @fig-pca, the parallel coordinate plot incorporates principal component analysis (PCA) components (PC1 to PC4) on the x-axis, with the scaled PCA values on the y-axis, color-coded by species (Adelie in red, Chinstrap in green, and Gentoo in blue).
PCA reduces dimensionality, meaning that each PC represents a combination of the original variables, simplifying complex relationships.
The lines between components show how each penguin transitions across these derived features, making it easier to observe general trends for each species in the reduced space.
Due to the numerous overlapping lines, overplotting remains an issue, though species-level patterns are visible, with distinct separations in specific PCs.
Outliers are challenging to detect due to the bundled nature of the lines.
While distribution shapes aren’t explicitly visible, the clustered points at each PC axis indicate each species' general concentration of values.
Though individual variability remains obscured, relationships between PCs and species are implied through color-coded clusters and trends, suggesting some separation between species along specific principal components.

## Cluster-Based and Hierarchical PCPs

Clustering techniques, such as spectral clustering, have been used in PCPs to group related data points and expose the underlying structure of high-dimensional datasets.
Clustering data and viewing the clusters in parallel coordinate plots makes detecting links between data points and cluster features easier.
This technique is beneficial for discovering patterns not immediately evident in raw data and providing insights into the natural grouping of data points [@zhao2012].
Several data reduction procedures, such as hierarchical clustering and principal component analysis, are used before plot generation to improve interpretability.
These strategies help to limit the number of dimensions visualized by focusing on the most important components that explain the majority of the variance in the data.

@zhao2012 work "Structure Revealing Techniques Based on Parallel Coordinates Plot" talks about how traditional parallel coordinate plots (PCPs) can't always show important patterns in complex, high-dimensional data because of problems like overplotting.
To deal with these problems, the writers suggest new ways to sort and cluster data specifically made for PCPs.
Using spectrum theory, they develop algorithms that group similar polylines and sort the axes to show hidden trends and correlations.
This makes the structure of the data easier to see.
A correlation-based sorting method is also added to arrange the axes to make relationships between variables stand out.
This makes it easier to see trends across dimensions.

The study also talks about view-range metrics, which use aggregation limits to help visualize data more clearly, even when the datasets are noisy.
Results from experiments show that these improvements make it much easier for PCPs to find useful patterns, trends, and correlations, which makes data analysis more efficient.
The results show that the suggested approaches improve PCPs' ability to analyze big, complicated datasets by showing important data structures that would be hard to see otherwise.

For multidimensional data, clustering and hierarchical PCPs allow data grouping and organized representation to enable easier cluster comparison.
Clustering and hierarchical visualization allow for data grouping, clarifying large datasets and enhancing comparison among groups.
Hierarchical clustering in PCPs offers a clearer, more organized data narrative by showing both the big picture and finer details, argues @fua1999.

@fua1999 introduced hierarchical PCPs, providing a zoomable interface to represent data clusters, allowing for detailed subset examination.
@geng2005 applied clustering in PCPs to group similar data points, highlighting clusters and making general patterns more accessible.
@poco2011 extended these techniques by incorporating hierarchical clustering for user-defined levels of granularity.

Although clusters simplify the data landscape, they can obscure individual data points.
@geng2005 observed that while useful for general patterns, clustering may bury unique or outlier data, potentially hiding significant findings in homogenous groups.

```{r}
#| echo: false
#| warning: false
#| label: fig-cluster
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Cluster-Based PCP"

# Select only numeric columns for clustering
penguins_numeric <- penguins_clean %>%
  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)

# Perform hierarchical clustering
penguins_dist <- dist(penguins_numeric)
penguins_hclust <- hclust(penguins_dist, method = "ward.D2")

# Cut the dendrogram into 3 clusters
penguins_clusters <- cutree(penguins_hclust, k = 3)

# Add cluster information to the dataset
penguins_clean$cluster <- as.factor(penguins_clusters)

# Create a parallel coordinate plot with clusters using ggparcoord
ggparcoord(
  data = penguins_clean,
  columns = 2:5, # Using the numeric columns
  groupColumn = "cluster",
  scale = "uniminmax", # Scale columns to range between [0, 1]
  alphaLines = 0.5, # Set transparency for lines
  #showPoints = TRUE, # Show points on lines
  mapping = ggplot2::aes(color = cluster)
) +
  scale_color_manual(values = c("1" = "#540D6E", "2" = "#219B9D", "3" = "#FF8000")) +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Cluster-Based",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle=45)) 
```

## Optimized Layout Algorithms

Layout algorithms for PCPs have been optimized to handle multi-dimensional data while minimizing line crossings, which enhances clarity and allows for smoother navigation.
Optimized layouts minimize line crossings, making PCPs more straightforward, particularly with extensive, high-dimensional data.
Effective layout design in PCPs can be the difference between comprehensibility and chaos. [@ankerst1998]
<!--[Is there a user study showing that this makes PCPs easier to read?]{.svp}-->
[Using three visualization techniques—including PCPs—the authors performed an empirical analysis contrasting conventional sequential layouts of dimensions with the suggested similarity-based arrangement.
Observations underlined that similarity configurations greatly improved readability by grouping comparable dimensions together, thereby facilitating the identification of patterns, correlations, and functional connections.]{.db}


[Automatic, heuristic PCP layout algorithms may produce results that do not]{.svp} align with user-defined analysis needs [@johansson2005].
Instead, @johansson2005 optimized the axis positioning algorithm to maximize the space between overlapping lines, improving data distinction. 
<!--[What does data distinction mean here? Are there experimental studies showing this?]{.svp}-->
[Data distinction, in the paper's context, is the capacity to distinguish and expose significant structures within intricate, crowded parallel coordinate displays. 
The authors solve a fundamental restriction of conventional parallel coordinates, in which overlapping lines hide trends, patterns, or data abnormalities. 
The work presents creative approaches for emphasizing several data properties, including cluster structures, local outliers, and intensity fluctuations, by including clustering, high-precision texturing, and transfer functions (TFs).]{.db}

[The article presents several experimental evaluations of cluster representation methods, including high-precision textures for cluster representation, transfer functions for enhanced visualization, local outlier detection, and feature animation. 
The high-precision cluster structure technique effectively represents clusters for datasets with up to 100,000 data points, allowing for easy identification of trends and sub-clusters. 
Transfer functions, such as logarithmic TFs, highlight varying aspects of clusters, revealing both high-density structures and subtle patterns. 
Local outlier detection uses interquartile range thresholds and enhanced transparency for clarity, enabling outliers to be visible even when embedded in dense clusters. 
Feature animation represents cluster variance and skewness, aiding in identifying asymmetric distributions or loose clusters. 
Variance-based animations guide users to regions requiring detailed investigation, reducing analysis complexity. 
Overall, these methods efficiently and effectively visualize clusters, enhancing the understanding of data structures and trends.]{.db}

[You need to show these different options and comment on the differences! Provide more detail about whether experimental evidence supports the approach. How does the approach in @ankerst1998 compare to the ggpcp ordering approach to minimize line crossings in one direction or another for categorical variables? I suspect that these algorithms change axis positioning, where ggpcp changes ordering within a category -- but you need to explicitly make that comparison.]{.svp}

Two visualization techniques, the Similarity Clustering Approach [@ankerst1998] and Clustered Parallel Coordinates and High-Precision Textures [@johansson2005], seek to improve interpretability by rearranging variables based on similarity rather than visual clutter reduction. 
The similarity clustering method emphasizes functional linkages and correlations and arranges dimensions based on their similarity; it does not explicitly seek to reduce line crossings, particularly for categorical data.

The clustered PCPs method solves clutter by aggregating data points into clusters and displaying them with high-precision textures. 
Rather than rearranging dimensions, it presents techniques to change the visual representation, such as transfer functions and outlier enhancements. 
This method reduces overplotting but does not reorganize dimensions to minimize crossings.

Strengths of `ggpcp` include direct visual optimization, computational simplicity for variable reordering, and precise handling of categorical data. 
It is more appropriate for big datasets with many dimensions when ggpcp's crossover optimization could still result in visual clutter. 
Furthermore, although similarity and cluster-based approaches manage numerical and categorical data by concentrating on trends and correlations, `ggpcp` is specialized for categorical variables.
Although each has advantages and disadvantages, the Similarity Clustering Approach and Clustered PCPs present distinct ways to view categorical data.

The `ggpcp` method prioritizes reducing line crossings in PCPs, offering a specific solution for categorical variables and improving interpretability. 
The approaches outlined in @ankerst1998 and @johansson2005 work emphasize dimension similarity and cluster representation to enhance the clarity of trends and structures in multidimensional datasets. 
Although they tackle similar issues, such as visual clutter, they are fundamentally distinct in their goals and methods. 
The `ggpcp` specializes in categorical line simplification, whereas the accompanying documents provide more comprehensive approaches for trend identification and dimensional relationships.


```{r}
#| echo: false
#| warning: false
#| label: fig-optimized
#| layout-valign: bottom
#| fig-cap: "While the PCP employs an optimized layout, its clarity is compromised by overplotting, lack of clustering or feature enhancement, and minimal reduction in line crossings. Clustering, high-precision textures, and transfer functions could greatly improve readability by simplifying data representation and directing attention to key features."

# Calculate the correlation matrix for optimized ordering
cor_matrix <- cor(penguins_clean %>%
                    select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g))

# Find the order of variables that maximizes the interpretability (based on correlation)
order_vars <- order.dendrogram(as.dendrogram(hclust(as.dist(1 - cor_matrix))))
penguins_optimized <- penguins_clean %>%
  select(all_of(order_vars), body_mass_g)

# Create a parallel coordinate plot with optimized layout using ggparcoord
ggparcoord(
  data = penguins_optimized,
  columns = c(1,3:5), # Using the optimized numeric columns
  groupColumn = "species",
  scale = "uniminmax", # Scale columns to range between [0, 1]
  alphaLines = 0.5, # Set transparency for lines
  #showPoints = TRUE, # Show points on lines
  mapping = ggplot2::aes(color = species)
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Optimized Layout",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle=45))
```

## Interactive and Dynamic

The addition of interactive features has substantially increased the usefulness of PCPs. 
Interactive PCPs enable users to dynamically change axes, reorder them, filter data based on specified criteria, and even invert axes to study data from various angles. 
These characteristics enable real-time data analysis, leading to a deeper understanding of patterns and correlations. 
Brushing and connecting allow users to highlight specific data points across many axes, whereas filtering hides lines that do not satisfy set criteria, decreasing clutter. 
The interactive characteristics of PCPs make them more amenable to exploratory data analysis, particularly in big and complicated datasets.

@heinrich2013 paper "State of the Art of Parallel Coordinates" thoroughly reviews visualization methods for parallel coordinates. 
It includes a taxonomy that groups the different approaches into different categories. 
The writers discuss various ways to model, see, and work with parallel coordinates. 
They also show how these methods can be used for everyday tasks in knowledge discovery, like sorting, clustering, and regression. 
Some of the most important advances are discussing geometric models, interpolation methods, and the point-line duality that makes up the basis of parallel coordinate plots. 
The study discusses a better understanding of data using density-based visualizations, axis ordering, and improvements such as brushing and bundling.

The study points out some problems with parallel coordinates, like overplotting and the need for good axis arrangement. 
It also suggests ways to get around these problems, such as clustering and density estimation. 
To make it easier to see patterns and understand data, researchers look into different ways to map it, such as curves, shapes, and density plots. 
The writers also talk about how parallel coordinates can be used in real life in engineering and the life sciences. 
This shows how useful and flexible they are for high-dimensional data visualization tasks.

Initially, modifications in parallel coordinate plots have dynamic features, which allow users to select, highlight, and filter data dimensions or specific data points in real-time. 
This has facilitated more precise and focused analysis within large datasets. 
Interaction allows users to engage with data intuitively, selectively focusing on areas of interest. 
Interactive filtering and brushing make it easier to identify patterns within large datasets, aiding discovery and hypothesis generation.

In 1990, @Wegman:1990 introduced fundamental techniques for interaction with multidimensional data, laying the groundwork for modern interactive PCPs. 
@siirtola2006 developed interactive features like brushing and linking to assist in dimensional analysis, making comparing attributes within large datasets easier. 
@inselberg2009 expanded these features by enabling dynamic filtering and axis rearrangement to tailor the display to user needs .

Increased complexity can overwhelm novice users, requiring more computational power and potentially slowing down analysis in high-dimensional datasets. 
@inselberg2009 noted, interactivity provides insight, it can lead to user fatigue in complex datasets due to the cognitive load required for multi-step filtering.

The live parallel coordinates plot on Plotly's ggplot2 page has many dynamic features that make looking at many different data types easy. Users can interact with the plot by moving their mouse across parallel axes to filter and separate specific data ranges. 
This lets them analyze only certain factors. 
When you move your mouse over a line, a tooltip appears with all of that line's numbers for each dimension. 
This makes it easier to understand large datasets. 
You can change the order of the axes by dragging them horizontally. 
This lets users adjust the dimensions to find hidden patterns and connections more easily.

Additionally, brush choices change the visualization in real-time, drawing more attention to relevant data paths and lessening the importance of others. 
This makes it easier to spot trends and outliers. 
These interactive parts work together to create an easy-to-use and adaptable way to see and analyze high-dimensional data in a parallel coordinates environment, [@plotly-pcp].

[Interactive Parallel Coordinate Plot](https://plotly.com/ggplot2/parallel-coordinates-plot/)

## Reordering and Axis Flipping

Adaptive reordering and axis flipping based on correlation measures or user-defined parameters can simplify the analysis of multidimensional relationships.
Reordering and axis flipping align more relevant dimensions, minimizing intersections and making relationships more interpretable.
Reordering transforms PCPs from a cluttered tangle into a roadmap of relationships.

@inselberg1990parallel proposed reordering to enhance interpretability, especially for highly interrelated variables.
@leblanc1990. introduced correlation-based reordering to position axes, reducing visual clutter by aligning more related dimensions.
Peng et al. (2004) enabled automatic reordering algorithms that flip axes according to user-defined weights, giving users more control over PCP readability.

Changing the order of the axes in parallel coordinate plots can make them easier to read by putting variables that are strongly correlated or connected by theme next to each other.
Moving the axes around reduces the amount of visual noise caused by crossings between variables that aren't strongly linked to each other.
This makes patterns and clusters stand out more.
The method of minimizing visual noise helps show hidden patterns in the data that would not be seen otherwise.
Researchers @johansson2005 and @wegman1997 all found that reordering is an important way to find connections in PCPs.
If you use well-thought-out reordering algorithms, you can see groups, trends, and outliers more clearly and avoid the feeling of overplotting.
This method works incredibly well for grouping data with many dimensions, making it easier to see.

Automatic reordering may reduce control over axis sequence, potentially misaligning dimensions relevant to a specific analytical question.
LeBlanc et al. pointed out that "the gain in interpretability through automated ordering can sometimes sacrifice user intention, misaligning axes crucial to the analysis context."

Axis flipping is a way to eliminate unnecessary line crossings and bring out trends in data by flipping the scale of one or more axes.
It works well when two PCP axes next to each other have negatively correlated factors.
This change allows different patterns to show up without too many crossings, giving a clearer picture of how the data is related.
Axis flipping is an important tool for dealing with negative correlations because it clarifies the connection between dimensions by reducing visual interference.
It also shows patterns of correlation that regular PCPs might miss, especially when working with datasets that have many factors that are unrelated to each other or are related in the opposite way.
Automated axis flipping methods can change plots on the fly based on data, which lowers the chance of mistakes when users interpret PCPs.
@dasgupta2010 studies both agree that axis flipping makes PCPs easier to understand by turning bad relationships into a more familiar shape [@dasgupta2010, @review].

Reordering and flipping axes based on correlation or user-defined parameters simplify the interpretation of complex relationships.
Dynamic reordering optimizes the alignment of correlated variables, making it easier to identify associations and trends across dimensions [@yuan2009].
This adaptation increases flexibility, allowing the plot to respond to specific analytical needs and user preferences.

Reordering and axis shifting work well together to fix the issue of overplotting in PCPs.
Rearranging linked variables makes them easier to see while flipping the axes reduces the number of times lines cross for negatively correlated variables.
Combining these two tools makes looking for patterns and exploring large datasets easier. [Citation needed]{.svp}
By using these methods, users can better understand how multiple variables are connected, which helps them make better decisions based on visual information.
Research shows [Citation needed]{.svp} that these methods make parallel coordinate plots easier to understand, more accurate, and better able to find groups and trends, all of which are very important for data analysts.
Continuous reordering can create inconsistency, as users may find it challenging to track relationships when axes are frequently altered.
"Constant reconfiguration of the axes can disrupt the analytical flow, making it difficult to build a stable mental model of the data structure" [@review].
Additionally, for datasets with minimal correlation, this method may offer limited value, as reordering may not result in clearer insights.

```{r}
#| echo: false
#| warning: false
#| label: fig-reordering
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Reordering and Axis Flipping in PCP"

# Reorder columns for better interpretability
penguins_reordered <- penguins_clean %>%
  select(species, flipper_length_mm, body_mass_g, bill_length_mm, bill_depth_mm)

# Create a parallel coordinate plot with reordering and axis flipping using ggparcoord
ggparcoord(
  data = penguins_reordered,
  columns = 2:5, # Using the reordered numeric columns
  groupColumn = 1,
  scale = "uniminmax", # Scale columns to range between [0, 1]
  alphaLines = 0.5, # Set transparency for lines
  #showPoints = TRUE, # Show points on lines
  mapping = ggplot2::aes(color = species)
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Reordering and Axis Flipping",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  ggplot2::scale_y_reverse() + # Flip the y-axis to observe the effect
  theme(axis.text.x = element_text(angle=45),
        axis.line.y = element_blank(), # Remove y-axis line
        axis.ticks.y = element_blank(), # Remove y-axis ticks
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) 

```

In @fig-reordering, the parallel coordinate map changes the variables by rearranging them and flipping the axes to show patterns and connections between the penguin species more clearly.
By flipping some axes and putting related factors closer together (like body mass and flipper length), the plot makes species-based trends stand out more: The Gentoo (blue) tends to have a shallower bill but a bigger body and longer flippers, while the Adelie (red) tends to have the opposite pattern.
Overplotting is still a problem, especially when values are close together in the middle ranges, making it hard to tell the difference between lines or peaks.
You can't see the distributions' exact forms, but the lines' density and spread along each axis give you a good idea of the range of values for each species.
The new order makes relationships between variables stand out more because the changes between the axes make species-based correlations in measurement trends more apparent.
However, individual relationships are still hard to see because lines cross.

# Handling Numerical Ties

Getting rid of ties in statistical data analysis has been hard for a long time for experts who want to make visualizations more clear. 
Previously, statistical methods focused on rank-based approaches and binary data handling. 
They often needed help with tied observations, which threw off distributions or made them more complex. 
Earlier methods for dealing with ties depended on changing ranking systems or adding rules to break ties, which could make results less accurate or easier to understand. 
As graphics data analysis grew, pioneers like Tukey, Chambers, and Cleveland created new ways to deal with ties visually. 
There are now several ways to deal with ties, such as slowly increasing the space between values. 
This makes the visualization more accurate and easier to understand without changing the structure of the data itself.

@tufte2001 work, particularly in "The Visual Display of Quantitative Information", is grounded in empirical evidence that shows how visual noise---any element that complicates or crowds the visual field---can impede the interpretation of data. 
He advocates for "graphical excellence," a design philosophy that presents data most clearly and efficiently. 
According to Tufte, separating tied values can reduce overlapping lines and decrease visual noise, making it easier for users to see patterns and accurately compare variables. 
Distancing tied values in complex visualizations can reduce interpretative difficulty by reducing visual noise, aiding rapid perception of quantitative relationships.

In complicated graphs like parallel coordinate plots, tied numbers often cause lines to overlap and hide other data points. 
Tufte calls this "chartjunk." 
He says that even minor cuts in visual noise can make it much easier for people to process information quickly and correctly. 
For example, if an extensive dataset has many similar or similar values, separating the tied values can help the viewer see trends and relationships without having to think too hard.

## Jittering Points

### Random Jittering of Data Points

Random jittering of data points is a standard method that adds a small random value to each tied observation.
This is especially helpful in scatter plots and dot plots to keep them from being too dense.
This approach, discussed in “Graphical Methods for Data Analysis," is easy to use and keeps the overall distribution, making things clearer without changing the data [@chambers1983].
One problem with random jittering is that it can hide exact numbers, making it hard to understand if the jittering distance is too big.
Experiments show that jittering makes visualizations much clearer when there are small overlaps, especially when used rarely.


In random jittering, we add a small random value (usually drawn from a uniform or normal distribution) to each tied observation to differentiate them.

For a set of tied values $x_1, x_2,...,x_n$, we define:

$$
x'_i = x_i + \epsilon_i
$$

where:

-   $x'_i$ is the jittered value of $x_i$,
-   $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$,
-   $\sigma$ is the standard deviation for the normal jitter.

```{r}
#| echo: false
#| warning: false
#| label: fig-random_jitter
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Random Jittering of Data Points in PCP"

# Add random jitter to numerical columns
penguins_jittered <- penguins
penguins_jittered$year <- jitter(penguins_jittered$year, amount = 2)

# Plot Parallel Coordinate Plot
ggparcoord(
  data = penguins_jittered,
  columns = c(3:6,8), # Select numerical columns
  groupColumn = 1, # Color by species
  scale = "center"
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Random Jittering",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  ggplot2::scale_y_reverse() + # Flip the y-axis to observe the effect
  theme(axis.text.x = element_text(angle=45),
        axis.line.y = element_blank(), # Remove y-axis line
        axis.ticks.y = element_blank(), # Remove y-axis ticks
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) 


```


### Rank Jittering (Rank Adjustment)

Rank jittering is another useful method for non-parametric analyses.
It changes the ranks of tied values instead of the real data values.
This makes it easier to see the ranks in non-parametric tests like the Wilcoxon signed-rank test.
@conover1999 “Practical Nonparametric Statistics” shows that rank jittering keeps the ordinal form of data while not changing the distribution much.
The best thing about rank jittering is that it keeps rank-based readings while making ties less noticeable.
However, the method can be hard to run on computers when dealing with big datasets, and making too many rank changes can accidentally change the test significance levels.
For these reasons, it is important to be careful when using rank jittering.

In rank jittering, we adjust the ranks of tied observations by adding a small random or systematic jitter to each tied rank.
This is usually applied to data that will undergo rank-based testing.

for tied ranks $R_1, R_2,...,R_n$:

$$
R'_i = R_ii + \delta_i
$$

where:

-   $R'_i$ is the jittered rank of $R_i$,
-   $\delta_i \sim \mathcal{N}(0,\sigma_{rank}^2)$
-   $\sigma_{rank}$ are small values ensuring that the adjustment is minor.

```{r}
#| echo: false
#| warning: false
#| label: fig-rank_jitter
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Rank Jittering of Data Points in PCP"

penguins_ranked <- penguins
penguins_ranked <- penguins_ranked[order(penguins_ranked$year, na.last = NA), ]
penguins_ranked$year_rank <- rank(penguins_ranked$year, na.last = "keep")

# Add rank-based jitter
penguins_ranked$year <- penguins_ranked$year + 
                                     penguins_ranked$year_rank * 0.5


# Plot Parallel Coordinate Plot
ggparcoord(
  data = penguins_ranked,
  columns = c(3:6,8), # Select numerical columns
  groupColumn = 1, # Color by species
  scale = "center"
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Rank Jittering",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  ggplot2::scale_y_reverse() + # Flip the y-axis to observe the effect
  theme(axis.text.x = element_text(angle=45),
        axis.line.y = element_blank(), # Remove y-axis line
        axis.ticks.y = element_blank(), # Remove y-axis ticks
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) 



```

A systematic way to use binary relationships in the data is to use ranking differences to break up ties. 
@ipkovich2021 writes that rank-based separation ensures that each tied observation is positioned proportionally to its relative rank, maintaining interpretability and visual clarity. 
This is one way to get rid of visual clutter in PCPs. 
This rank-based method assigns spacing factors based on rank differences to ensure that overlapped lines are spread evenly across the plot. 
The method works well to reduce overlap and maintain the order of the items, but it can be very time-consuming to run on big datasets with many ties. 
In any case, it works incredibly well for showing categorical or ordinal data where order is important.


### Deterministic Jittering (Fixed Perturbation)

When you need to be clear, deterministic jitter gives you a structured way to do it by adding a small fixed value (epsilon) to each tied observation instead of depending on chance.
This method explained in more detail in @david1977 “Exploratory Data Analysis”, is useful when random noise is not wanted because it ensures that each observation is moved in a way that can be predicted and repeated.
Deterministic jitter is easier to understand than random jitter, but it needs to be done carefully so that it doesn't introduce fake patterns, especially when scaling is involved.
Deterministic jittering is a good choice for random noise that doesn't affect the integrity of the distribution when used carefully.

In deterministic jittering, a small, fixed value $\epsilon$ is added to each tied value to systematically spread them apart without introducing randomness.

For tied values $x_1, x_2,...,x_n$, we use:

$$
x'_i = x_i + i\epsilon
$$

where:

-   $x'_i$ is the adjusted value of $x_i$,
-   $i$ is the indeex of the tied observation (e.g., $i = 1,2,...,n$),
-   $\epsilon$ is a small fixed distance chosen based on the data scale.

```{r}
#| echo: false
#| warning: false
#| label: fig-deterministic_jitter
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Deterministic Jittering of Data Points in PCP"

# Add fixed perturbation
penguins_fixed <- penguins
penguins_fixed$year <- ifelse(duplicated(penguins_fixed$year),
                                           penguins_fixed$year + 0.5,
                                           penguins_fixed$year)


# Plot Parallel Coordinate Plot
ggparcoord(
  data = penguins_fixed,
  columns = c(3:6, 8), # Select numerical columns
  groupColumn = 1, # Color by species
  scale = "center"
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Deterministic Jittering",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  ggplot2::scale_y_reverse() + # Flip the y-axis to observe the effect
  theme(axis.text.x = element_text(angle=45),
        axis.line.y = element_blank(), # Remove y-axis line
        axis.ticks.y = element_blank(), # Remove y-axis ticks
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) 


```


Jittering can be a common way to deal with ties in parallel coordinate plots (PCPs). 
It involves adding small amounts of random or predictable noise to tied observations. 
@swayne2003 handle ties by jittering as an intuitive way to separate tied values in 2-D correlation tours without distorting their statistical relationships. 
While jittering makes things easier to see, its main flaw is that it needs to be easier to figure out the right amount of noise. 
Similarly, @few2008solutions calls jittering a practical solution to the perceptual challenges caused by overplotting.
They point out that it can tell the difference between lines that overlap while keeping the structure of the whole picture. 
Their review of the current solution stresses that jittering can improve visual clarity in dense datasets, mainly when carefully regulated.


## Mean or Median Splitting

One more popular method is mean or median splitting, which gives values tied to their mean or median values.
This method works especially well for symmetric distributions and is perfect for images that need to show things centrally, like box plots.
@wilcox2017 “Modern Statistics for the Social and Behavioral Sciences” shows that splitting the mean or median gives a fair view, especially for boxplot data.
This method works best with datasets with few ties because it can hide the range and variability within tied numbers if used too much.

In mean or median splitting, we replace each tied value with the mean or median of the tied group.
This technique centers tied observations on their central tendency.

For set of tied values $x_1, x_2,...,x_n$ let:

\begin{center}
$x_{mean} = \frac{1}{n}\sum^{n}_{i=1}x_i$ or $x_{median} = median(x_1, x_2,...,x_n)$
\end{center}

Then:

\begin{center}
$x'_i = x_{mean}$ or $x'_i = x_{median}$
\end{center}

This replaces each tied value $x_i$ with the computed mean or median, clustering them at a central point.


```{r}
#| echo: false
#| warning: false
#| label: fig-mean_jitter
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Mean or Median Splitting of Data Points in PCP"

# Mean splitting for ties
penguins_mean <- penguins
mean_year <- mean(penguins_mean$year, na.rm = TRUE)
penguins_mean$year <- ifelse(duplicated(penguins_mean$year),
                                          penguins_mean$year + (penguins_mean$year - mean_year) * 0.1,
                                          penguins_mean$year)


# Plot Parallel Coordinate Plot
ggparcoord(
  data = penguins_mean,
  columns = c(3:6,8), # Select numerical columns
  groupColumn = 1, # Color by species
  scale = "center"
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with Mean or Median Splitting",
                y = "Scaled Values",
                x = "Variables") +
  theme(legend.position = "bottom") +
  ggplot2::scale_y_reverse() + # Flip the y-axis to observe the effect
  theme(axis.text.x = element_text(angle=45),
        axis.line.y = element_blank(), # Remove y-axis line
        axis.ticks.y = element_blank(), # Remove y-axis ticks
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) 



```


## Kernel Density Estimation (KDE) with Bandwidth Adjustment

Kernel density estimation (KDE) with bandwidth adjustment is a powerful method for density visualizations.
It spreads out overlapping observations by giving each tied value a slightly different weight.
This method works especially well in density plots because it evens data distributions without changing the original numbers.
@silverman2018 work in “Density Estimation for Statistics and Data Analysis” shows how KDE with changed bandwidths correctly shows multimodal distributions, reducing errors caused by tied values.
KDE works, but you must be careful when choosing the bandwidth because making the wrong choice can lead to misleading visual representations, especially when files are very concentrated.


For KDE with bandwidth adjustment, each data point is used to estimate a probability density function (PDF) by smoothing over a specified bandwidth $h$, which can be adjusted to account for ties.

The KDE for a set of observations $x_1, x_2,...,x_n$ is:

$$
\hat{f}(x) = \frac{1}{nh}\sum^{n}_{i=1}K(\frac{x-x_i}{h})
$$

where:

-   $\hat{f}(x)$ is the estimiated density at $x$,
-   $K(\cdot)$ is a kernel function
-   $h$ is the bandwidth, chosen to spread tied values slightly by selecting a larger $h$ for clusters of tied observations.

The choice of bandwidth $h$ controls the amount of smoothing, which helps visually differentiate tied values.

```{r}
#| echo: false
#| warning: false
#| label: fig-kde_jitter
#| layout-valign: bottom
#| fig-subcap: 
#|   - "KDE with Bandwidth Adjustment of Data Points in PCP"

# KDE transformation
penguins_kde <- penguins
penguins_kde$year_density <- density(penguins_kde$year, na.rm = TRUE)$bw

# Add KDE smoothed values
penguins_kde$year <- penguins_kde$year + penguins_kde$year_density * 0.1


# Plot Parallel Coordinate Plot
ggparcoord(
  data = penguins_kde,
  columns = c(3:6,8), # Select numerical columns
  groupColumn = 1, # Color by species
  scale = "center"
) +
  penguin_scale_color() +
  ggplot2::theme_minimal() +
  ggplot2::labs(title = "Parallel Coordinate Plot with KDE with Bandwidth Adjustment",
                x = "Variables") +
  theme(legend.position = "bottom") +
  ggplot2::scale_y_reverse() + # Flip the y-axis to observe the effect
  theme(axis.text.x = element_text(angle=45),
        axis.line.y = element_blank(), # Remove y-axis line
        axis.ticks.y = element_blank(), # Remove y-axis ticks
        axis.text.y = element_blank(),
        axis.title.y = element_blank()) 


```


Advanced visual methods, such as depth cues and density adjustments, are used to deal with ties by making PCPs look like they are in three dimensions.  
When combined with line transparency, Depth Cue Parallel Coordinates (DCPC) provide an effective means of visually distinguishing overlapping trends with teemporal binning that employs perception-based coloring and concepts, [@johansson2007]. 
This method was created to deal with the problem of having a lot of temporal data. 
Their primary focus is on using transparency and brightness changes to layer connected information, which lets users see data as if it were in three-dimensional space. 
This method keeps the original data and ensures each line can remain identified. 
Although the technique works, it requires advanced rendering tools that might not be easy to find in all visualization settings.


Each way to show data with ties has unique benefits depending on the type of display and the type of data.
Random and predictable jittering, for example, works well in scatterplots because they stop the plot from getting too big and show hidden patterns.
KDE and rank-based methods, on the other hand, work well in density plots and ordinal visualizations.
Researchers can improve visual clarity by using only some of these methods, ensuring the representation is correct and easy to understand.
However, when parallel coordinate plots are used, where multiple directions are plotted simultaneously, the old ways of dealing with ties only sometimes work.
In this case, adding the distance between values tied across multiple axes could show patterns and connections that are hard to see because the lines overlap.
By creating a new method for parallel coordinate plots that adds space between ties in a planned way, we could see data trends and links across dimensions more clearly while keeping the dataset's integrity.
This new idea would make it easier to understand complex, multidimensional data.

# Handling Ties in PCPs

The idea of adding distance to visual ties has changed over time as methods for visualizing data have improved to find a balance between detail and clarity in large datasets.
In the early days of visualization, especially in network analysis and multidimensional scaling, closeness and alignment were the main ways to show patterns and ties between things.
However, as data grew, visual clutter became a big problem.
This was especially true in parallel coordinates plots (PCPs), where lines and intersections that overlapped made it hard to see the underlying data structure.
To fix this problem, researchers started looking into ways to give representational parts of these graphs more "visual distance." They did this using methods like line bundling and changing the spacing between lines to avoid overplotting.
"In PCP, participants had to re-order the axes to examine all dimensions, whereas in SPLOM it is straightforward to identify correlation. 
This result confirms earlier studies showing PCP participants struggle to identify correlation due to the complexity of cross-line patterns," [@chang2018a]
This shows how separating visual elements can help with cognitive grouping without breaking up the continuity of the data.
In PCPs, early studies showed that visual distance could make intersections clearer and improve the overall shape of individual paths.
@guo2005a say, "...nested-means approach reduces overlapping problems and ensures the mean value of each variable is always at the midpoint on each axis."
This theory has shaped later developments, showing visual distance as a spatial tool and a key cognitive help in data interpretation needed to get around today's more complex visualizations.

## Axis Reordering

As mentioned in the section on modifications, axis reordering changes the order of dimensions to reduce the number of times lines cross between variables in PCPs, a type of data display.
This method makes clusters easier to see by lining up visually linked variables, cutting down on clutter, and making patterns stand out more without overlapping.
One benefit is that it works well for datasets where certain factors are strongly related, making clusters and trends easier to read without changing the data.
Changing the variable order for clarity rather than data continuity could make it hard to tell if the data is in order.
@blumenschein2020 say that reordering based on correlation minimizes unnecessary line intersections, allowing natural clusters to emerge. This shows how important changing the arrangement can be for seeing clearly.
Studies in this area have shown that changing the order of dimensions makes it much easier to find patterns in datasets where dimensions are linked.
However, it only works well on datasets with dimensions tied together.

```{r}
#| echo: false
#| warning: false
#| label: fig-axis
#| layout-valign: bottom
#| fig-subcap: 
#|   - "PCP Without Axis Reordering"
#|   - "PCP With Skewness Axis Reordering"

penguins <- na.omit(penguins)

# Example of a parallel coordinate plot without handling ties explicitly
p1 <- ggparcoord(
  data = penguins,
  columns = 3:6,  # Columns for numerical features (bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
  groupColumn = 1,
  scale = "uniminmax",  # Scaling to [0, 1] for better visualization
) +
  #ggtitle("Without Axis Reordering")  +
  theme_minimal() +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle=45)) +
  penguin_scale_color()

# Example of a parallel coordinate plot with axis reordering to minimize tie overlap
p2 <- ggparcoord(
  data = penguins,
  columns = 3:6,  # Columns for numerical features
  groupColumn = 1,
  scale = "uniminmax",
  order = "skewness") + # skewness axis reordering to handle ties and improve interpretability
  penguin_scale_color()+   
  #ggtitle("With Skewness Axis Reordering") +
  theme_minimal() +
  theme(legend.position = "bottom")  +
  theme(axis.text.x = element_text(angle=45))

# Try using patchwork. Think about removing the legend and adding that in the section.
grid.arrange(p1,p2, ncol=2)

```

## Line Bundling


By putting similar lines into "bundles," line bundling cuts down on unnecessary crossings and clutter.
This method works well in dense PCPs, where many lines cross and make it hard to see individual patterns.
Heinrich et al. say that "bundling lines based on value similarity reduces cognitive strain and enhances visual coherence in complex datasets." This shows how bundling can make complex data easier to understand without losing larger patterns.
Experiments have shown that bundling makes large datasets easier to read, which makes it easier to find general trends even when the data is heavily packed [@heinrich2011].
It makes things easier to understand by collecting similar data lines; this works especially well for numerical ties where values are close but not exactly the same.
Bundling, on the other hand, can hide individual lines within a group, making it hard to tell one data point from another.


## Quantization and Aggregation

Quantization, also known as Quantized Generalized Parallel Coordinate Plots or QGPCP, groups close-together values into bins.
This makes it easier to see patterns in tightly packed data.
This method cuts down on the number of lines that cross each other, which makes it great for big datasets.
Moustafa states, "By quantizing close data points, we can minimize overplotting and improve discernment of overall trends." This shows that removing some details can make data easier to read.
Quantization can keep important patterns while lowering noise, as tests on real-world data show.
However, it may hide small but important differences in the data [@moustafa2009].
It makes the image easier to understand by removing small details.
This is helpful for high-dimensional datasets with a lot of line overlap.
However, detail can be lost during aggregation, making this method less useful for situations where exact data representation is needed.

```{r}
#| echo: false
#| warning: false
#| label: fig-qgpcp
#| layout-valign: bottom
#| fig-subcap: 
#|   - "Quantized Generalized Parallel Coordinate Plots"
#|   
# Remove rows with NA values for simplicity
penguins_clean <- penguins %>% 
  na.omit()

# Discretize/Quantize numerical columns and encode categorical variables
penguins_encoded <- penguins_clean %>%
  mutate(
    species = as.factor(species),  # Encode species as numeric
    island = as.numeric(as.factor(island)),    # Encode island as numeric
    sex = as.numeric(as.factor(sex)),          # Encode sex as numeric
    bill_length_mm = cut(bill_length_mm, breaks = 5, labels = FALSE),  # Quantize bill_length_mm
    bill_depth_mm = cut(bill_depth_mm, breaks = 5, labels = FALSE),    # Quantize bill_depth_mm
    flipper_length_mm = cut(flipper_length_mm, breaks = 5, labels = FALSE), # Quantize flipper_length_mm
    body_mass_g = cut(body_mass_g, breaks = 5, labels = FALSE)         # Quantize body_mass_g
  )

# Create the parallel coordinate plot
ggparcoord(
  data = penguins_encoded,
  columns = 2:8,    # Use all columns, including encoded categorical variables
  groupColumn = 1,  # Grouping by species
  scale = "uniminmax", # Scale columns to the same range
  #showPoints = TRUE,   # Show points on the lines
  alphaLines = 0.6     # Transparency of lines
) + 
  theme_minimal() + 
  labs(title = "Quantized Parallel Coordinate Plot of Palmer Penguins Dataset",
       x = "Attributes",
       y = "Quantized/Encoded Values") +
  theme(legend.position = "bottom")  +
  theme(axis.text.x = element_text(angle=45)) +
    penguin_scale_color()
```

In @fig-qgpcp, the overlapping of the Quantized Parallel Coordinate Plot makes it harder to find specific individuals.
Quantization makes the visualization even easier by putting numbers into groups.
However, this needs to improve some of the accuracy of the original data.
The plot can help you see big patterns, like how some species may follow similar paths across many traits, but it doesn't show exactly how they are related or where they live.
Using jittering or interactive plots to cut down on overplotting could make things clearer and make figuring out how factors relate easier.

## Color-Coding and Transparency

By changing the color and transparency of lines, you can tell them apart based on density, categories, or values.
This makes it easier to tell the difference between PCP lines that meet.
Firat et al. say, "Line brushing and transparency adjustments improve the gestalt of the data clusters." This means that changes in how we see things help us group data cognitively without changing the structure.
Firat et al. say that user tests have shown that PCPs with color-coded and clear lines are easier to understand and recognize patterns, especially for differentiating categorical data [@firat2023].
This tool can be used to handle both numerical and categorical ties.
The color and transparency options make it easy to visually separate clusters without changing where the lines are placed.
However, this method might only work for plots with little information because even changes in color can't fully fix heavy overplotting.

## Edge Bundling for Categorical Data


Edge bundling, which comes from network visualization, groups paths for category values in PCPs.
This method works best with categorical data because it lets you visually break up ties without changing other factors.
Palmas et al. state that "bundling lines with similar orientations prevents overplotting, allowing clearer identification of dominant trends within complex data." This means bundling can help find trends even in spaces with many dimensions.
Bundling made finding categorical clusters in complex PCPs easier and decreased users' work [@palmas2014].
Edge bundling is an excellent way to clear up clutter and make categorical groups easier to see.
It lets users see categorical differences without messing up numerical relationships.
On the other hand, each bundle may hide small details, making them less useful for datasets that need a lot of precision.

## Stacked Histograms and Density Plots

This change adds histograms or density plots on top of PCPs to show how category data is distributed in terms of frequency.
This lets users see how distributions change without adding extra line clutter.
According to @bok2020, adding histogram overlays gives categorical context, making variable distributions easier to see without affecting line continuity.
This means that frequency-based cues can help you understand categorical distributions without adding more visual clutter.
Clear frequency-based differentiation in categorical data without too much line overlap improves the visual understanding of distributions when used with PCPs.
Real-world category dataset tests showed that histogram overlays made it easier to differentiate between data sets and see what they meant.
Adding histograms makes things look more complicated, which could be too much for some people if used with other PCP changes.


## Perceptual Cues (Line Thickness, Texture, etc.)

By changing the lines' width, texture, or other visual properties, you can tell them apart from overlapping lines.
This helps people see patterns and tell values apart without separating them physically.
According to @chang2018, perceptual variations in lines not only differentiate data but also enhance recognition of broader patterns. 
These cues help with cognitive grouping and pattern differentiation, even in complicated datasets.
They work well in areas with a lot of visual information because they let users see trends without changing the structure or order of the data.
They are helpful for both numerical and categorical ties.
Studies with users showed that changes in how people saw things made finding patterns and trends in large, complex datasets easier.
This made it easier for PCPs to look around.
In areas with many people, it could be useful because overlapped lines can make it harder to see.

# References
